{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Pipeline:\n",
    "This is a basic pipeline implementation. In real-life data science, scenario data would \n",
    "need to be prepared first then applied pipeline for rest processes. Building quick and \n",
    "efficient machine learning models is what pipelines are for. Pipelines are high in demand\n",
    "as it helps in coding better and extensible in implementing big data projects. Automating \n",
    "the applied machine learning workflow and saving time invested in redundant preprocessing work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages of using Pipeline:\n",
    "\n",
    "Automating the workflow being iterative.\n",
    "\n",
    "Easier to fix bugs \n",
    "\n",
    "Production Ready\n",
    "\n",
    "Clean code writing standards\n",
    "\n",
    "Helpful in iterative hyperparameter tuning and cross-validation evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "It ensures reusability of the model by reducing the redundant part, thereby speeding up \n",
    "the process. This could prove to be very effective during the production workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "iris_df=load_iris()\n",
    "# Split\n",
    "X_train,X_test,y_train,y_test=train_test_split(iris_df.data,iris_df.target,test_size=0.3,random_state=0)\n",
    "#make pipeline\n",
    "pipeline_lr=Pipeline([('scalar1',StandardScaler()),\n",
    "                     ('pca1',PCA(n_components=2)),('lr_classifier',LogisticRegression(random_state=0))])\n",
    "model = pipeline_lr.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Multiple Pipelines to Find the Model with the Best Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Accuracy:0.8666666666666667\n",
      "Decision Tree Test Accuracy:0.9111111111111111\n",
      "Support Vector Machine Test Accuracy:0.9111111111111111\n",
      "K Nearest Neighbor Test Accuracy:0.9111111111111111\n",
      "RandomForestClassifier Test Accuracy:0.9111111111111111\n",
      "GaussianNB() Test Accuracy:0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "pipeline_lr=Pipeline([('scalar1',StandardScaler()),\n",
    "                     ('pca1',PCA(n_components=2)), \n",
    "                     ('lr_classifier',LogisticRegression())])\n",
    "pipeline_dt=Pipeline([('scalar2',StandardScaler()),\n",
    "                     ('pca2',PCA(n_components=2)),\n",
    "                     ('dt_classifier',DecisionTreeClassifier())])\n",
    "pipeline_svm = Pipeline([('scalar3', StandardScaler()),\n",
    "                      ('pca3', PCA(n_components=2)),\n",
    "                      ('clf', svm.SVC())])\n",
    "pipeline_knn=Pipeline([('scalar4',StandardScaler()),\n",
    "                     ('pca4',PCA(n_components=2)),\n",
    "                     ('knn_classifier',KNeighborsClassifier())])\n",
    "\n",
    "pipeline_rf = Pipeline([('scalar5',StandardScaler()),\n",
    "                     ('pca5',PCA(n_components=2)),\n",
    "                     ('rf_classifier',RandomForestClassifier())])\n",
    "\n",
    "pipeline_nb = Pipeline([('scalar6',StandardScaler()),\n",
    "                     ('pca6',PCA(n_components=2)),\n",
    "                     ('nb_classifier',GaussianNB())])\n",
    "\n",
    "MultinomialNB()\n",
    "\n",
    "pipelines = [pipeline_lr, pipeline_dt, pipeline_randomforest, pipeline_knn, pipeline_rf,pipeline_nb]\n",
    "pipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'Support Vector Machine',3:'K Nearest Neighbor',4:'RandomForestClassifier',5:'GaussianNB()'}\n",
    "\n",
    "for pipe in pipelines:\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "for i,model in enumerate(pipelines):\n",
    "    print(\"{} Test Accuracy:{}\".format(pipe_dict[i],model.score(X_test,y_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with best accuracy:Decision Tree\n"
     ]
    }
   ],
   "source": [
    "best_accuracy=0.0\n",
    "best_classifier=0\n",
    "best_pipeline=\"\"\n",
    "\n",
    "for i,model in enumerate(pipelines):\n",
    "    if model.score(X_test,y_test)>best_accuracy:\n",
    "        best_accuracy=model.score(X_test,y_test)\n",
    "        best_pipeline=model\n",
    "        best_classifier=i\n",
    "print('Classifier with best accuracy:{}'.format(pipe_dict[best_classifier]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning in Pipeline\n",
    "With pipelines, you can easily perform a grid-search over a set of parameters for each step \n",
    "of this meta-estimator to find the best performing parameters. To do this you first need to\n",
    "create a parameter grid for your chosen model. One important thing to note is that you need \n",
    "to append the name that you have given the classifier part of your pipeline to each parameter\n",
    "name. In my code above I have called this ‘randomforestclassifier’ so I have added \n",
    "randomforestclassifier__ to each parameter. Next, I created a grid search object which \n",
    "includes the original pipeline. When I then call fit, the transformations are applied to \n",
    "the data, before a cross-validated grid-search is performed over the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipe = make_pipeline((RandomForestClassifier()))\n",
    "grid_param = [\n",
    "{\"randomforestclassifier\": [RandomForestClassifier()],\n",
    "\"randomforestclassifier__n_estimators\":[10,100,1000],\n",
    " \"randomforestclassifier__max_depth\":[5,8,15,25,30,None],\n",
    " \"randomforestclassifier__min_samples_leaf\":[1,2,5,10,15,100],\n",
    "\"randomforestclassifier__max_leaf_nodes\": [2, 5,10]}]\n",
    "gridsearch = GridSearchCV(pipe, grid_param, cv=5, verbose=0,n_jobs=-1) \n",
    "best_model = gridsearch.fit(X_train,y_train)\n",
    "best_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges in using Pipeline:\n",
    "Need Proper data cleaning, \n",
    "Data Exploration and Analysis and \n",
    "Efficient feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "Logistic regression is an extension to the linear regression algorithm. The details of the\n",
    "linear regression algorithm are discussed in Learn regression algorithms using Python and\n",
    "scikit-learn. In a logistic regression algorithm, instead of predicting the actual continuous\n",
    "value, we predict the probability of an outcome. To achieve this, a logistic function is applied\n",
    "to the outcome of the linear regression. The logistic function is also referred to as a sigmoid\n",
    "function. This outputs a value between 0 and 1. Then, we select a line that depends on the use \n",
    "case. Any data point with a probability value above the line is classified into the class \n",
    "represented by 1. The data point below the line is classified into the class represented by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_name = \"lrc\"\n",
    "lrc = LogisticRegression(random_state=0,multi_class='auto',solver = 'lbfgs',max_iter=1000)\n",
    "lrc_model = Pipeline(steps=['preprocessor']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Apply all classification algorithm by adding pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm.classes import OneClassSVM\n",
    "from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
    "from sklearn.neighbors.classification import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from sklearn.linear_model.ridge import RidgeClassifierCV\n",
    "from sklearn.linear_model.ridge import RidgeClassifier\n",
    "from sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier    \n",
    "from sklearn.gaussian_process.gpc import GaussianProcessClassifier\n",
    "from sklearn.ensemble.voting_classifier import VotingClassifier\n",
    "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import DPGMM\n",
    "from sklearn.mixture import GMM \n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import VBGMM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  baseline models\n",
    "When developing a machine learning model for a project it is sensible to create a baseline \n",
    "model first. This model should be in essence a ‘dummy’ model such as one that always predicts\n",
    "the most frequently occurring class. This provides a baseline on which to benchmark your \n",
    "‘intelligent’ model so that you can ensure that it is performing better than random results \n",
    "for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42105263157894735"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "\n",
    "# Create target vector and feature matrix\n",
    "X, y = iris.data, iris.target\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "# Create dummy classifer\n",
    "dummy = DummyClassifier(strategy='uniform', random_state=1)\n",
    "\n",
    "# \"Train\" model\n",
    "dummy.fit(X_train, y_train)\n",
    "# Get accuracy score\n",
    "dummy.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn solutions for various problesm\n",
    "https://www.neuraxio.com/blogs/news/whats-wrong-with-scikit-learn-pipelines\n",
    "https://www.neuraxle.org/stable/scikit-learn_problems_solutions.html#problem-defining-the-search-space-hyperparameter-distributions    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
